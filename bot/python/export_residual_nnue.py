"""Export a SimpleNNUE checkpoint to the residual-nnue-v1 binary format."""

from __future__ import annotations

import argparse
import json
import struct
import sys
from pathlib import Path
from typing import Iterable, Tuple

import torch
from torch import nn

# Make the repository root importable so we can reuse the SimpleNNUE definition.
REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.bot.model import ResidualBlock, SimpleNNUE  # noqa: E402


def _tensor_bytes(t: torch.Tensor) -> bytes:
    """Return the tensor data as contiguous float32 bytes."""
    return (
        t.detach()
        .to(dtype=torch.float32, device="cpu")
        .contiguous()
        .numpy()
        .tobytes()
    )


def _write_linear_record(fh, layer: nn.Linear) -> None:
    weight = layer.weight
    bias = layer.bias
    out_dim, in_dim = weight.shape
    fh.write(struct.pack("<i", 1))  # type id
    fh.write(struct.pack("<i", in_dim))
    fh.write(struct.pack("<i", out_dim))
    fh.write(_tensor_bytes(weight))
    fh.write(_tensor_bytes(bias))


def _write_linear_params(fh, layer: nn.Linear) -> None:
    weight = layer.weight
    bias = layer.bias
    out_dim, in_dim = weight.shape
    fh.write(struct.pack("<i", in_dim))
    fh.write(struct.pack("<i", out_dim))
    fh.write(_tensor_bytes(weight))
    fh.write(_tensor_bytes(bias))


def _write_layernorm_record(fh, layer: nn.LayerNorm) -> None:
    dim = int(layer.normalized_shape[0])
    eps = float(layer.eps)
    fh.write(struct.pack("<i", 2))  # type id
    fh.write(struct.pack("<i", dim))
    fh.write(struct.pack("<f", eps))
    fh.write(_tensor_bytes(layer.weight))
    fh.write(_tensor_bytes(layer.bias))


def _write_residual_record(fh, block: ResidualBlock) -> None:
    dim = block.lin1.out_features
    fh.write(struct.pack("<i", 3))  # type id
    fh.write(struct.pack("<i", dim))
    _write_linear_params(fh, block.lin1)
    _write_linear_params(fh, block.lin2)
    ln_dim = int(block.norm.normalized_shape[0])
    ln_eps = float(block.norm.eps)
    fh.write(struct.pack("<i", ln_dim))
    fh.write(struct.pack("<f", ln_eps))
    fh.write(_tensor_bytes(block.norm.weight))
    fh.write(_tensor_bytes(block.norm.bias))


def _collect_layers(model: SimpleNNUE) -> Iterable[Tuple[str, nn.Module]]:
    for layer in model.backbone:
        if isinstance(layer, nn.Linear):
            yield ("linear", layer)
        elif isinstance(layer, nn.LayerNorm):
            yield ("layernorm", layer)
        elif isinstance(layer, ResidualBlock):
            yield ("residual", layer)
        else:
            # Drop ReLU/Dropout during export; inference re-applies ReLU.
            continue
    yield ("linear", model.output_head)


def export_residual_nnue(input_path: str | Path, output_path: str | Path) -> Path:
    ckpt_path = Path(input_path)
    output_path = Path(output_path)
    if not ckpt_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")

    checkpoint = torch.load(ckpt_path, map_location="cpu")
    if "model_state" not in checkpoint or "model_config" not in checkpoint:
        raise ValueError(
            "Checkpoint must contain model_state and model_config (generated by train_nnue.py)."
        )
    config = checkpoint["model_config"]
    input_dim = int(config.get("input_dim", 0))
    hidden_dims = config.get("hidden_dims")
    dropout = float(config.get("dropout", 0.0))
    residual_repeats = config.get("residual_repeats")

    model = SimpleNNUE(
        input_dim=input_dim,
        hidden_dims=hidden_dims,
        dropout=dropout,
        residual_repeats=residual_repeats,
    )
    model.load_state_dict(checkpoint["model_state"])
    model.eval()

    layers = list(_collect_layers(model))
    record_count = sum(1 for kind, _ in layers if kind in ("linear", "layernorm", "residual"))
    header = json.dumps(
        {
            "format": "residual-nnue-v1",
            "input_dim": input_dim,
            "layer_count": record_count,
            "hidden_dims": hidden_dims,
            "residual_repeats": residual_repeats,
            "dropout": dropout,
            "source": str(ckpt_path),
        }
    ).encode("utf-8")

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("wb") as fh:
        fh.write(struct.pack("<I", len(header)))
        fh.write(header)
        for kind, layer in layers:
            if kind == "linear":
                _write_linear_record(fh, layer)
            elif kind == "layernorm":
                _write_layernorm_record(fh, layer)
            elif kind == "residual":
                _write_residual_record(fh, layer)

    return output_path


def main() -> None:
    parser = argparse.ArgumentParser(description="Export SimpleNNUE checkpoint to residual-nnue-v1 binary.")
    parser.add_argument(
        "-i",
        "--input",
        type=Path,
        default=Path("bot/python/epoch1.pt"),
        help="Path to the SimpleNNUE checkpoint (.pt) file.",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=Path("bot/python/simple_nnue.bin"),
        help="Destination path for the residual-nnue-v1 binary.",
    )
    args = parser.parse_args()
    out_path = export_residual_nnue(args.input, args.output)
    print(f"Wrote SimpleNNUE residual binary to {out_path}")


if __name__ == "__main__":
    main()
